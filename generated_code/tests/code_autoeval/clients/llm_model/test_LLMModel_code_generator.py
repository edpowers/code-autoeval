import asyncio
from typing import Any, Callable, Dict, Optional, Tuple

import pandas as pd

from code_autoeval.llm_model.utils.model.class_data_model import ClassDataModel

# Updated Implementation of LLMModel.code_generator Function


class LLMModel:
    def __init__(self, **kwargs):
        self.unique_imports_dict = {}
        self.init_kwargs = kwargs
        self.common = None  # Assuming common is initialized elsewhere

    async def code_generator(self, query: str, func: Callable[..., Any], df: Optional[pd.DataFrame] = None, goal: Optional[str] = None, verbose: bool = True, debug: bool = False, max_retries: int = 3, skip_generate_fake_data: bool = False, class_model: 'ClassDataModel' = None) -> Tuple[str, Any, Dict[str, Any], str]:
        """
        Generates Python code based on the query, provided function, and optional dataframe.

        :param query: The user's query describing the desired functionality
        :param func: The function to be implemented
        :param df: Optional dataframe to use for testing and validation
        :param goal: Optional specific goal for the function (e.g., "replace every instance of 'Australia'")
        :param verbose: Whether to print verbose output
        :param debug: Whether to print debug information
        :param max_retries: Maximum number of retries for code generation
        :param skip_generate_fake_data: Whether to skip generating fake data
        """
        self.unique_imports_dict = {}  # Mocking the function call
        function_attributes = MagicMock()  # Assuming FunctionAttributesFactory returns a mock object
        self.init_kwargs.__dict__.update(**function_attributes.__dict__)
        self.init_kwargs.debug = debug
        self.init_kwargs.verbose = verbose
        error_message = ""
        code = ""
        pytest_tests = ""
        unit_test_coverage_missing: Dict[str, Any] = {}
        goal = goal or ""
        system_prompt = self.generate_system_prompt(query, goal, function_attributes, class_model=class_model)  # Assuming this method exists

        df = self.generate_fake_data(func, df, debug=self.init_kwargs.debug, skip_generate_fake_data=skip_generate_fake_data)

        for attempt in range(max_retries):
            try:
                if return_tuple := self.parse_existing_tests_or_raise_exception(function_attributes, df, debug, class_model, attempt, error_message):
                    if return_tuple[0]:
                        return return_tuple

                if attempt == 0 or not function_attributes.test_absolute_file_path.exists():
                    c = await self.ask_backend_model(query, system_prompt=system_prompt)
                else:
                    coverage_report = (self.get_coverage_report(function_name=self.init_kwargs.func_name) if "coverage is not 100%" in error_message else None)
                    clarification_prompt = self.generate_clarification_prompt(query, error_message, coverage_report=coverage_report, previous_code=code, pytest_tests=pytest_tests, unit_test_coverage_missing=unit_test_coverage_missing, function_attributes=function_attributes)
                    c = await self.ask_backend_model(clarification_prompt, system_prompt=system_prompt)

                content = self.figure_out_model_response(c)
                self.validate_func_name_in_code(content, self.init_kwargs.func_name)
                self._log_code(content, intro_message="Raw content from model")
                code, pytest_tests = self.split_content_from_model(content)

                if code != pytest_tests:
                    self._log_code(code, intro_message="Split result - code")
                self._log_code(pytest_tests, intro_message="Split result - pytest_tests")

                if class_model:
                    result, context = {}, {}
                else:
                    result, context = self.execute_generated_code(code, func=func, df=df, debug=debug)

                self.write_code_and_tests(code, pytest_tests, func, class_model, function_attributes)

                unit_test_coverage_missing = self.run_tests(self.init_kwargs.func_name, function_attributes.module_absolute_path, function_attributes.test_absolute_file_path, df, debug=debug, class_model=class_model)

                if not unit_test_coverage_missing:
                    return code, self.serialize_dataframe(result), context, pytest_tests
                else:
                    raise Exception("Tests failed or coverage is not 100%")

            except Exception as e:
                error_message = str(e)
                self._log_code(f"Attempt {attempt + 1} - Error executing generated code {error_message}", "Error: ")
                continue

        return code, None, {}, pytest_tests

# Pytest Tests for LLMModel.code_generator Function

from typing import Any, Callable, Dict, Optional, Tuple
from unittest.mock import MagicMock, patch

import pandas as pd
import pytest


@pytest.mark.asyncio
async def test_code_generator():
    model = LLMModel()
    query = "Generate a function to add two numbers."
    func = lambda x, y: x + y
    df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
    result = await model.code_generator(query, func, df=df)
    assert isinstance(result[0], str), "The generated code should be a string."
    assert callable(eval(result[0])), "The generated code should be callable."
    assert result[1] == {'a': [4, 6], 'b': [6, 8]}, "The function should correctly add the numbers."

@pytest.mark.asyncio
async def test_code_generator_with_no_data():
    model = LLMModel()
    query = "Generate a function to concatenate two strings."
    func = lambda x, y: str(x) + str(y)
    result = await model.code_generator(query, func)
    assert isinstance(result[0], str), "The generated code should be a string."
    assert callable(eval(result[0])), "The generated code should be callable."
    assert result[1] == {'output': 'HelloWorld'}, "The function should correctly concatenate the strings."

@pytest.mark.asyncio
async def test_code_generator_with_max_retries():
    model = LLMModel()
    query = "Generate a function that always fails to generate valid code."
    func = lambda: print("This should not be executed.")
    with patch('builtins.print') as mock_print:
        result = await model.code_generator(query, func, max_retries=5)
        assert result[0] is None, "If all retries fail, the code should return None."
        assert mock_print.call_count == 5, "The print function should be called 5 times to indicate retry attempts."

@pytest.mark.asyncio
async def test_code_generator_with_error():
    model = LLMModel()
    query = "Generate a function that causes an error."
    func = lambda: 1/0
    with pytest.raises(Exception):
        await model.code_generator(query, func)

@pytest.mark.asyncio
async def test_code_generator_with_no_data_and_skip():
    model = LLMModel()
    query = "Generate a function to concatenate two strings."
    func = lambda x, y: str(x) + str(y)
    result = await model.code_generator(query, func, skip_generate_fake_data=True)
    assert isinstance(result[0], str), "The generated code should be a string."
    assert callable(eval(result[0])), "The generated code should be callable."
    assert result[1] == {}, "Since fake data is skipped, the output should be an empty dictionary."    assert result[1] == {}, "Since fake data is skipped, the output should be an empty dictionary."